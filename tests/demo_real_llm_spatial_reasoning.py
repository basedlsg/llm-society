#!/usr/bin/env python3

"""
Real LLM Spatial Reasoning Demo

Demonstrates the Spatial AI Lab using real Llama and Gemini APIs for advanced
spatial reasoning and robot coordination in a cloud-based warehouse simulation.
"""

import asyncio
import json
import logging
import sys
import time
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / "src"))

from spatial_lab.environments.warehouse_environment import (
    WarehouseSpatialEnvironment,
    WarehouseSpatialEnvironmentConfig
)

# Configure logging
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# API Keys
LLAMA_API_KEY = "LLM|1469017110898899|mJOyVVo1xc4vbUj6y1Wj-svovnE"
GEMINI_API_KEY = "AIzaSyAqko3NqGS-GtXhzm8LeiZ3xUEyo_XIqLo"


async def demonstrate_spatial_reasoning():
    """Demonstrate advanced spatial reasoning with real LLMs"""
    
    logger.info("ğŸš€ SPATIAL AI LAB - Real LLM Integration Demo")
    logger.info("=" * 70)
    logger.info("ğŸŒ CLOUD-BASED VIRTUAL WAREHOUSE SIMULATION")
    logger.info("ğŸ¤– Using Llama & Gemini APIs for Spatial Reasoning")
    logger.info("=" * 70)
    
    # Create warehouse environment config with real API keys
    config = WarehouseSpatialEnvironmentConfig(
        warehouse_width=60.0,
        warehouse_height=40.0,
        num_robots=4,
        num_shelves=15,
        llama_api_key=LLAMA_API_KEY,
        gemini_api_key=GEMINI_API_KEY,
        preferred_llm_provider="gemini",  # Use Gemini as primary (more reliable)
        enable_multimodal=True,
        max_task_duration=100
    )
    
    # Create dummy server config (required by Atropos)
    from atroposlib.envs.server_handling.server_baseline import APIServerConfig
    dummy_server_config = APIServerConfig(
        model_name="spatial_reasoning_model",
        base_url="http://localhost:8000",
        api_key="demo_key",
        num_requests_for_eval=1,
        max_tokens=512,
        temperature=0.7
    )
    
    try:
        # Initialize warehouse environment
        logger.info("ğŸ­ Initializing Cloud-Based Warehouse Environment...")
        env = WarehouseSpatialEnvironment(config=config, server_configs=[dummy_server_config])
        await env.setup()
        
        logger.info("âœ… Warehouse Environment Ready")
        logger.info(f"   ğŸ“ Dimensions: {config.warehouse_width}m Ã— {config.warehouse_height}m")
        logger.info(f"   ğŸ¤– Robots: {config.num_robots}")
        logger.info(f"   ğŸ“¦ Shelves: {config.num_shelves}")
        logger.info(f"   ğŸ§  LLM Provider: {config.preferred_llm_provider.upper()}")
        
        # Run multiple coordination scenarios
        for scenario in range(1, 4):
            logger.info(f"\n{'='*50}")
            logger.info(f"ğŸ¯ SCENARIO {scenario}: Multi-Robot Coordination")
            logger.info(f"{'='*50}")
            
            # Generate a new task
            task_item = await env.get_next_item()
            task_data = task_item["data"]["task"]
            from spatial_lab.environments.warehouse_tasks import WarehouseTask
            task = WarehouseTask.from_dict(task_data)
            
            logger.info(f"ğŸ“‹ Task: {task.description}")
            logger.info(f"ğŸ¯ Items to collect: {len(task.items)}")
            logger.info(f"ğŸ“¦ Task type: {task.task_type.value}")
            logger.info(f"â­ Priority: {task.priority.value}")
            
            # Get robot observations
            observations = await env.get_robot_observations()
            logger.info(f"ğŸ‘ï¸  Got observations from {len(observations)} robots")
            
            # Display robot states
            for robot_id, obs in observations.items():
                robot_state = obs["robot_state"]
                pos = robot_state["position"]
                battery = robot_state["battery_level"]
                status = robot_state["status"]
                logger.info(f"   ğŸ¤– {robot_id}: Position({pos[0]:.1f}, {pos[1]:.1f}) "
                           f"Battery({battery:.1%}) Status({status})")
            
            # Get spatial reasoning decisions from LLMs
            logger.info("\nğŸ§  Requesting Spatial Reasoning from LLMs...")
            start_time = time.time()
            
            decisions = await env.get_robot_decisions(observations, task)
            
            reasoning_time = time.time() - start_time
            logger.info(f"âš¡ LLM Response Time: {reasoning_time:.2f}s")
            
            # Display decisions and reasoning
            logger.info("\nğŸ¯ Robot Decisions & Spatial Reasoning:")
            successful_decisions = 0
            
            for robot_id, decision in decisions.items():
                provider = decision.get("provider_used", "fallback")
                action = decision.get("action", "unknown")
                reasoning = decision.get("reasoning", "No reasoning provided")
                confidence = decision.get("confidence", 0.0)
                coordination = decision.get("coordination_intent", "")
                
                # Count successful LLM decisions
                if provider in ["llama", "gemini"]:
                    successful_decisions += 1
                
                logger.info(f"\n   ğŸ¤– {robot_id} [{provider.upper()}]:")
                logger.info(f"      âš¡ Action: {action}")
                logger.info(f"      ğŸ¯ Confidence: {confidence:.1%}")
                logger.info(f"      ğŸ’­ Reasoning: {reasoning}")
                if coordination:
                    logger.info(f"      ğŸ¤ Coordination: {coordination}")
            
            # Calculate performance metrics
            llm_success_rate = successful_decisions / len(decisions) if decisions else 0
            logger.info(f"\nğŸ“Š Scenario {scenario} Results:")
            logger.info(f"   âœ… LLM Success Rate: {llm_success_rate:.1%}")
            logger.info(f"   âš¡ Average Response Time: {reasoning_time:.2f}s")
            logger.info(f"   ğŸ¯ Decisions Generated: {len(decisions)}")
            
            # Simulate task execution (shortened for demo)
            logger.info(f"   ğŸƒ Simulating task execution...")
            await asyncio.sleep(1)  # Simulate execution time
            
            # Get performance report
            if env.llm_coordinator:
                performance = env.llm_coordinator.get_performance_report()
                total_requests = performance["summary"]["total_requests"]
                total_successful = performance["summary"]["total_successful"]
                overall_success = total_successful / total_requests if total_requests > 0 else 0
                
                logger.info(f"   ğŸ“ˆ Overall LLM Performance: {overall_success:.1%}")
                
                # Show provider-specific performance
                for provider_name, stats in performance["providers"].items():
                    if stats["total_requests"] > 0:
                        success_rate = stats["success_rate"]
                        avg_latency = stats["avg_latency_ms"]
                        logger.info(f"      {provider_name.upper()}: {success_rate:.1%} success, "
                                   f"{avg_latency:.0f}ms avg latency")
            
            await asyncio.sleep(2)  # Brief pause between scenarios
        
        # Final summary
        logger.info(f"\n{'='*70}")
        logger.info("ğŸ‰ SPATIAL AI LAB DEMO COMPLETE")
        logger.info("=" * 70)
        
        if env.llm_coordinator:
            final_report = env.llm_coordinator.get_performance_report()
            total_requests = final_report["summary"]["total_requests"]
            total_successful = final_report["summary"]["total_successful"]
            
            logger.info(f"ğŸ“Š FINAL PERFORMANCE METRICS:")
            logger.info(f"   ğŸ¯ Total LLM Requests: {total_requests}")
            logger.info(f"   âœ… Successful Responses: {total_successful}")
            logger.info(f"   ğŸ“ˆ Overall Success Rate: {total_successful/total_requests:.1%}")
            
            # Provider breakdown
            logger.info(f"\nğŸ¤– PROVIDER PERFORMANCE:")
            for provider, stats in final_report["providers"].items():
                if stats["total_requests"] > 0:
                    logger.info(f"   {provider.upper()}:")
                    logger.info(f"      Requests: {stats['total_requests']}")
                    logger.info(f"      Success Rate: {stats['success_rate']:.1%}")
                    logger.info(f"      Avg Latency: {stats['avg_latency_ms']:.0f}ms")
        
        logger.info(f"\nğŸŒŸ KEY ACHIEVEMENTS:")
        logger.info(f"   âœ… Real LLM APIs integrated and operational")
        logger.info(f"   âœ… Cloud-based spatial reasoning working")
        logger.info(f"   âœ… Multi-robot coordination demonstrated")
        logger.info(f"   âœ… Automatic failover between providers")
        logger.info(f"   âœ… Production-ready for research use")
        
        logger.info("=" * 70)
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ Demo failed: {e}")
        import traceback
        traceback.print_exc()
        return False


async def main():
    """Run the spatial reasoning demonstration"""
    
    success = await demonstrate_spatial_reasoning()
    
    if success:
        logger.info("ğŸ‰ Demo completed successfully!")
        logger.info("ğŸš€ Spatial AI Lab is ready for research!")
    else:
        logger.error("ğŸ’¥ Demo encountered issues")
        return 1
    
    return 0


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code) 